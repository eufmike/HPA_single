{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.0+cu118\n",
      "Torchvision version: 0.15.1+cu118\n",
      "CUDA is available: True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "# from torch.utils.data.dataset import Datasets\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "input_ch = 4 # 3 or 4\n",
    "weight_decay = 1e-5\n",
    "max_epochs = 100\n",
    "val_interval = 1\n",
    "batch_size = 128\n",
    "# GPUs = [3, 2]\n",
    "GPUs = [3, 0, 1, 2]\n",
    "num_workers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = now.strftime(\"%d_%m_%Y_%H_%M\")\n",
    "data_dir = Path('/dlab/ldrive/CBT/USLJ-DSDE_DATA-I10008/shihch3/projects/HPA_single_data')\n",
    "checkpoint_dir = data_dir.joinpath('checkpoints', timestamp)\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "log_dir = data_dir.joinpath('log', timestamp)\n",
    "writer = SummaryWriter(log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/runtime_env/miniconda3/envs/mspytorch/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from HPASCDataset import HPASCDataset\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "    transforms.Resize((2048, 2048)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), \n",
    "    ])  \n",
    "\n",
    "# transform = transforms.Compose(\n",
    "#     [transforms.ToTensor()])\n",
    "\n",
    "datadir= Path('/dlab/ldrive/CBT/USLJ-DSDE_DATA-I10008/BenchmarkDatasets/hpa-single-cell-image-classification')\n",
    "train_dataset_dir = datadir.joinpath('train')\n",
    "train_csv = datadir.joinpath('train.csv')\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "def prepare(rank, world_size, batch_size=32, pin_memory=False, num_workers=0):\n",
    "    HPA_dataset = HPASCDataset(\n",
    "                    input_csv = train_csv, \n",
    "                    root = train_dataset_dir, \n",
    "                    split = 'train', \n",
    "                    transform = transform,\n",
    "                    input_ch = input_ch, \n",
    "                    n_class = 19, \n",
    "                    # debug_size = 100,\n",
    "                    )\n",
    "    torch.manual_seed(1947)\n",
    "    train_ds, val_ds = random_split(HPA_dataset, [0.9, 0.1])\n",
    "    print(\"train_ds size:\", len(train_ds))\n",
    "    print(\"val_ds size:\", len(val_ds))\n",
    "    train_sampler = DistributedSampler(train_ds, num_replicas=world_size, rank=rank, shuffle=False, drop_last=False)\n",
    "    train_loader = DataLoader(train_ds, \n",
    "                              batch_size=batch_size, \n",
    "                              pin_memory=pin_memory, \n",
    "                              num_workers=num_workers, \n",
    "                              drop_last=False, \n",
    "                              shuffle=False, \n",
    "                              sampler=train_sampler)\n",
    "    val_sampler = DistributedSampler(val_ds, num_replicas=world_size, rank=rank, shuffle=False, drop_last=False)\n",
    "    val_loader = DataLoader(val_ds, \n",
    "                              batch_size=batch_size, \n",
    "                              pin_memory=pin_memory, \n",
    "                              num_workers=num_workers, \n",
    "                              drop_last=False, \n",
    "                              shuffle=False, \n",
    "                              sampler=val_sampler)\n",
    "    return train_ds, val_ds, train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = next(dataiter)\n",
    "# images, labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(images[0]))\n",
    "# print(images[0].shape)\n",
    "# print(images[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show images\n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "# print(' '.join(f'{labels[j]}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation device: cuda:3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device(f'cuda:{GPUs[0]}')\n",
    "print(f\"Computation device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_ch, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 509 * 509, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 19)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# model = Net()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(rank, world_size):\n",
    "    # setup the process groups\n",
    "    setup(rank, world_size)\n",
    "    # prepare the dataloader\n",
    "    train_ds, val_ds, train_loader, val_loader = prepare(rank, world_size)\n",
    "    model = Net().to(rank)\n",
    "    model = DDP(model, device_ids=[rank], output_device=rank, find_unused_parameters=True)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = max_epochs)\n",
    "\n",
    "    best_metric_epoch = -1\n",
    "    epoch_loss_values = []\n",
    "    min_val_loss = 0.0\n",
    "\n",
    "    for epoch in range(max_epochs):  # loop over the dataset multiple times\n",
    "        epoch_start = time.time()\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "        epoch_loss = 0.0\n",
    "        step = 0\n",
    "        for i, batch_data  in tqdm(enumerate(train_loader, 0)):\n",
    "            step_start = time.time()\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = batch_data \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            print(\n",
    "                f\"{step}/{len(train_ds) // train_loader.batch_size}\"\n",
    "                f\", train_loss: {loss.item():.4f}\"\n",
    "                f\", step time: {(time.time() - step_start):.4f}\"\n",
    "            )\n",
    "            step += 1\n",
    "            batch_calculation = 5\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        writer.add_scalar('Loss/train', epoch_loss, epoch+1)\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:  # print every 2000 mini-batches\n",
    "            epoch_val_loss = 0.0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_step = 0\n",
    "                for val_data in val_loader:\n",
    "                    val_inputs, val_labels = val_data\n",
    "                    val_inputs = val_inputs.to(device)\n",
    "                    val_labels = val_labels.to(device)\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    val_loss = criterion(val_outputs, val_labels)\n",
    "                    epoch_val_loss += val_loss.item()\n",
    "                    val_step += 1\n",
    "            \n",
    "            epoch_val_loss /= val_step\n",
    "            writer.add_scalar('Loss/valid', val_loss, epoch+1)\n",
    "            \n",
    "            if min_val_loss > epoch_val_loss and epoch > 0: \n",
    "                print(f'Validation Loss Decreased({min_val_loss:.6f}--->{epoch_val_loss:.6f}) \\t Saving The Model')\n",
    "                min_val_loss = epoch_val_loss\n",
    "                best_metric_epoch = epoch + 1\n",
    "                print(f'Best Metric Epoch: {best_metric_epoch}')\n",
    "                # Saving State Dict\n",
    "                torch.save(model.state_dict(), checkpoint_dir.joinpath('best_checkpoint.pth'))\n",
    "                \n",
    "            else:\n",
    "                min_val_loss = val_loss\n",
    "                print(min_val_loss)\n",
    "\n",
    "    print('Finished Training')\n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/runtime_env/miniconda3/envs/mspytorch/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "  File \"/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/runtime_env/miniconda3/envs/mspytorch/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "  File \"/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/runtime_env/miniconda3/envs/mspytorch/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/runtime_env/miniconda3/envs/mspytorch/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/runtime_env/miniconda3/envs/mspytorch/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/runtime_env/miniconda3/envs/mspytorch/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'main' on <module '__main__' (built-in)>\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'main' on <module '__main__' (built-in)>\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'main' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "ProcessExitedException",
     "evalue": "process 1 terminated with exit code 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/shihch3/code/p/python/HPA_single/script/resnet_multicls_multigpu.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdsdegpu01/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/shihch3/code/p/python/HPA_single/script/resnet_multicls_multigpu.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdsdegpu01/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/shihch3/code/p/python/HPA_single/script/resnet_multicls_multigpu.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# suppose we have 3 gpus\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdsdegpu01/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/shihch3/code/p/python/HPA_single/script/resnet_multicls_multigpu.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     world_size \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m    \n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdsdegpu01/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/shihch3/code/p/python/HPA_single/script/resnet_multicls_multigpu.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     mp\u001b[39m.\u001b[39;49mspawn(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdsdegpu01/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/shihch3/code/p/python/HPA_single/script/resnet_multicls_multigpu.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         main,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdsdegpu01/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/shihch3/code/p/python/HPA_single/script/resnet_multicls_multigpu.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         args\u001b[39m=\u001b[39;49m(world_size),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdsdegpu01/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/shihch3/code/p/python/HPA_single/script/resnet_multicls_multigpu.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         nprocs\u001b[39m=\u001b[39;49mworld_size\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdsdegpu01/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/shihch3/code/p/python/HPA_single/script/resnet_multicls_multigpu.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     )\n",
      "File \u001b[0;32m/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/runtime_env/miniconda3/envs/mspytorch/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:239\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    235\u001b[0m     msg \u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mThis method only supports start_method=spawn (got: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    236\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mTo use a different start_method use:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    237\u001b[0m            \u001b[39m'\u001b[39m\u001b[39m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m start_method)\n\u001b[1;32m    238\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(msg)\n\u001b[0;32m--> 239\u001b[0m \u001b[39mreturn\u001b[39;00m start_processes(fn, args, nprocs, join, daemon, start_method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mspawn\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/runtime_env/miniconda3/envs/mspytorch/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:197\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[39mreturn\u001b[39;00m context\n\u001b[1;32m    196\u001b[0m \u001b[39m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39;49mjoin():\n\u001b[1;32m    198\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/runtime_env/miniconda3/envs/mspytorch/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:149\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[39mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    141\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mprocess \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m terminated with signal \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    142\u001b[0m             (error_index, name),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m             signal_name\u001b[39m=\u001b[39mname\n\u001b[1;32m    147\u001b[0m         )\n\u001b[1;32m    148\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m         \u001b[39mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    150\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mprocess \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m terminated with exit code \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    151\u001b[0m             (error_index, exitcode),\n\u001b[1;32m    152\u001b[0m             error_index\u001b[39m=\u001b[39merror_index,\n\u001b[1;32m    153\u001b[0m             error_pid\u001b[39m=\u001b[39mfailed_process\u001b[39m.\u001b[39mpid,\n\u001b[1;32m    154\u001b[0m             exit_code\u001b[39m=\u001b[39mexitcode\n\u001b[1;32m    155\u001b[0m         )\n\u001b[1;32m    157\u001b[0m original_trace \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_queues[error_index]\u001b[39m.\u001b[39mget()\n\u001b[1;32m    158\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-- Process \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m terminated with the following error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m error_index\n",
      "\u001b[0;31mProcessExitedException\u001b[0m: process 1 terminated with exit code 1"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # suppose we have 3 gpus\n",
    "    world_size = 3    \n",
    "    mp.spawn(\n",
    "        main,\n",
    "        args=(world_size),\n",
    "        nprocs=world_size\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
