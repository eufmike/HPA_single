{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.0+cu118\n",
      "Torchvision version: 0.15.1+cu118\n",
      "CUDA is available: True\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "# print(os.getcwd())\n",
    "# sys.path.append(os.getcwd())\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir= Path('/dlab/ldrive/CBT/USLJ-DSDE_DATA-I10008/BenchmarkDatasets/hpa-single-cell-image-classification')\n",
    "train_dataset_dir = datadir.joinpath('train')\n",
    "train_csv = datadir.joinpath('train_select_1K.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from func.HPASCDataset import HPASCDataset, TrDataset\n",
    "from func.trainer import load_train_objs\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "input_ch_ct = 4\n",
    "debug_size = 10\n",
    "n_class = 19\n",
    "HPA_dataset = HPASCDataset(\n",
    "                        input_csv = train_csv, \n",
    "                        root = train_dataset_dir, \n",
    "                        input_ch_ct = input_ch_ct,\n",
    "                        n_class = n_class, \n",
    "                        debug_size = debug_size,\n",
    "                        )\n",
    "\n",
    "dataloader = DataLoader(\n",
    "                    HPA_dataset,\n",
    "                    batch_size=1,\n",
    "                    pin_memory=True,\n",
    "                    num_workers=1,\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 2048, 2048])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2048, 2048)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "No image, video, mask or bounding box was found in the sample",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/shihch3/code/p/python/HPA_single/imgviz.ipynb Cell 4\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blinuxbox/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/shihch3/code/p/python/HPA_single/imgviz.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m transform_1 \u001b[39m=\u001b[39m v2\u001b[39m.\u001b[39mRandomResizedCrop(size \u001b[39m=\u001b[39m (\u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m), antialias\u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blinuxbox/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/shihch3/code/p/python/HPA_single/imgviz.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m composed_transform \u001b[39m=\u001b[39m Compose([transform_1])\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blinuxbox/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/shihch3/code/p/python/HPA_single/imgviz.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m out \u001b[39m=\u001b[39m transform_1(image)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinuxbox/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/shihch3/code/p/python/HPA_single/imgviz.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(out\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/runtime_env/miniconda3/envs/mspytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/runtime_env/miniconda3/envs/mspytorch/lib/python3.9/site-packages/torchvision/transforms/v2/_transform.py:40\u001b[0m, in \u001b[0;36mTransform.forward\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_inputs(flat_inputs)\n\u001b[1;32m     39\u001b[0m needs_transform_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_needs_transform_list(flat_inputs)\n\u001b[0;32m---> 40\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_params(\n\u001b[1;32m     41\u001b[0m     [inpt \u001b[39mfor\u001b[39;49;00m (inpt, needs_transform) \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(flat_inputs, needs_transform_list) \u001b[39mif\u001b[39;49;00m needs_transform]\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     44\u001b[0m flat_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m     45\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform(inpt, params) \u001b[39mif\u001b[39;00m needs_transform \u001b[39melse\u001b[39;00m inpt\n\u001b[1;32m     46\u001b[0m     \u001b[39mfor\u001b[39;00m (inpt, needs_transform) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[1;32m     47\u001b[0m ]\n\u001b[1;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n",
      "File \u001b[0;32m/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/runtime_env/miniconda3/envs/mspytorch/lib/python3.9/site-packages/torchvision/transforms/v2/_geometry.py:270\u001b[0m, in \u001b[0;36mRandomResizedCrop._get_params\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_params\u001b[39m(\u001b[39mself\u001b[39m, flat_inputs: List[Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[0;32m--> 270\u001b[0m     height, width \u001b[39m=\u001b[39m query_spatial_size(flat_inputs)\n\u001b[1;32m    271\u001b[0m     area \u001b[39m=\u001b[39m height \u001b[39m*\u001b[39m width\n\u001b[1;32m    273\u001b[0m     log_ratio \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_ratio\n",
      "File \u001b[0;32m/dlab/ldrive/CBT/USLJ-DSDE-I10007/DSDE/runtime_env/miniconda3/envs/mspytorch/lib/python3.9/site-packages/torchvision/transforms/v2/utils.py:45\u001b[0m, in \u001b[0;36mquery_spatial_size\u001b[0;34m(flat_inputs)\u001b[0m\n\u001b[1;32m     36\u001b[0m sizes \u001b[39m=\u001b[39m {\n\u001b[1;32m     37\u001b[0m     \u001b[39mtuple\u001b[39m(get_spatial_size(inpt))\n\u001b[1;32m     38\u001b[0m     \u001b[39mfor\u001b[39;00m inpt \u001b[39min\u001b[39;00m flat_inputs\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mor\u001b[39;00m is_simple_tensor(inpt)\n\u001b[1;32m     43\u001b[0m }\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sizes:\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo image, video, mask or bounding box was found in the sample\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(sizes) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     47\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFound multiple HxW dimensions in the sample: \u001b[39m\u001b[39m{\u001b[39;00msequence_to_str(\u001b[39msorted\u001b[39m(sizes))\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: No image, video, mask or bounding box was found in the sample"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import v2, Compose, Resize\n",
    "from func.transform import tv_v1_train\n",
    "\n",
    "dataiter = iter(dataloader)\n",
    "image, label = next(dataiter)\n",
    "print(image.shape)\n",
    "print(image.shape)\n",
    "# image_new = tv_v1_train(image)\n",
    "transform_1 = v2.RandomResizedCrop(size = (224, 224), antialias= True)\n",
    "composed_transform = Compose([transform_1])\n",
    "out = transform_1(image)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 30)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "orig_img = Image.open(Path('./assets/astronaut.jpg'))\n",
    "\n",
    "resized_imgs = [v2.Resize(size=size)(orig_img) for size in (30, 50, 100, orig_img.size)]\n",
    "# plot([orig_img] + padded_imgs)\n",
    "print(resized_imgs[0].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
